---
title: "Wrangling review and a few new things"
author: "Casey O'Hara, Nathan Grimes, Allison Horst"
format: 
  html:
    code-fold: show
    toc: true
editor: visual
execute:
  echo: true
  message: false
  warning: false
---

# Objectives

-   Reminders of reproducible workflows
-   Review some data wrangling basics (`dplyr` & `tidyr`, `group_by`, `summarize`, `filter`, `select`)
-   Introduce a couple of new functions (`separate`, `drop_na`)
-   Remember `ggplot`?
-   A map in R

# Setup

-   Check in on installation and PAT - reminder that if you set it to expire in 30 days, don't freak out when it seems to stop working in a month!
-   **Fork** this repo from GitHub (`oharac/esm244_w2024_lab1`) to create a version in your own user account
-   **Clone** to create a version controlled R project on your local computer
-   Create a new Quarto document in the project called `lab_1_<yourinitials>` (e.g. mine will be `lab_1_cco`)
-   Adjust the yaml header for chunk options, code folding, table of contents

# Attach packages

In the setup chunk in your RMarkdown document, attach the following packages:

-   `tidyverse`
-   `here`
-   `sf`
-   `tmap`

\*Note: you may need to install these packages if you don't already have them (recall: `install.packages("packagename")`)

```{r setup}
library(tidyverse)
library(here)
library(sf)
library(tmap)
```

# Read in the data

The data you'll use (to start) is within the `data/sfo_trees` subfolder. Use the `here` package to read in the `sfo_trees.csv` file.

```{r}
sfo_trees_df <- read_csv(here("data", "sfo_trees", "sfo_trees.csv")) %>%
  janitor::clean_names()
```

About the data: SF trees data are from the [SF Open Data Portal](https://data.sfgov.org/City-Infrastructure/Street-Tree-List/tkzw-k3nq). See more information from Thomas Mock and TidyTuesday [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-01-28). Data updated 1/10/2024, with certain columns dropped to reduce file size.

Check out the data using exploratory functions (e.g. `View()`, `names()`, `summary()`, etc.). Remember that those probably do **not** belong in your .Rmd code chunks (if you don't need a record, you can either comment it out or put it in the Console).

# Part 1: Data exploration and wrangling review

## Example 1

Find counts of observations by `legal_status`. Then select the statuses with the five highest tree counts.

### Pseudocode

### Functional Code

```{r example 1a}
# Way 1: group_by %>% summarize %>% n
sfo_trees_df %>% 
  group_by(legal_status) %>% 
  summarize(tree_count = n()) %>%
  ungroup()

# Way 2: Same thing (+ a few other dplyr functions)
top_5_status <- sfo_trees_df %>% 
  filter(!is.na(legal_status)) %>% 
  ### drop_na(legal_status) %>% ### same thing!
  group_by(legal_status) %>% 
  summarize(tree_count = n()) %>%
  ungroup() %>%
  slice_max(tree_count, n = 5) %>% 
  arrange(-tree_count)
```

Make a graph of top 5 from above (take time to try on your own first):

```{r example 1b}
ggplot(data = top_5_status, 
       aes(x = legal_status, 
           # x = fct_reorder(legal_status, tree_count),
           y = tree_count)) +
  geom_col() +
  labs(y = "Tree count", x = "Legal Status") +
  # coord_flip() + ### can also just swap x and y in the aes()
  theme_minimal() 
```

## Example 2

Only keep observations where legal status is `Permitted Site` and caretaker is `MTA` or `DPW`. Store as `permitted_mta_dpw`.

### Pseudocode

### Functional code

```{r ex 2}
permitted_mta_dpw <- sfo_trees_df %>% 
  filter(legal_status == "Permitted Site" & caretaker %in% c("MTA", "DPW"))
```

Here note the use of the *and* operator in the filter, and compare use of `%in%` vs `==` operator here!

## Example 3

Only keep observations of oak and pine trees, then only keep columns `species`, `legal_status`, `plant_date`, `latitude` and `longitude`. Note which category each tree falls into, as column `type`. Store as `oak_pine_df`.

### Pseudocode

### Functional code

The `stringr` package contains a bunch of useful functions for finding & working with strings (e.g. words). One is `str_detect()` to detect a specific string within in a column.

Here note the use of an *or* operator in the `filter` function.

```{r ex 3a}
oak_pine_df <- sfo_trees_df %>% 
  filter(str_detect(species, "Oak") | str_detect(species, 'Pine')) %>% 
  select(species, legal_status, plant_date, latitude, longitude) %>%
  mutate(type = ifelse(str_detect(species, "Oak"), 'Oak', 'Pine'))
```

Make a little graph of locations (note R doesn't know these are spatial yet). Which trees are oaks vs pines?

```{r ex 3b}
ggplot(data = oak_pine_df, aes(x = longitude, y = latitude, color = type)) + 
  geom_point() +
  theme_minimal() +
  theme(axis.title = element_blank()) +
  labs(color = 'Tree type', 
       caption = 'Locations of Oaks and Pines in San Francisco')
```

## Example 4

Load a list of CA native species.

```{r ex 4a}
ca_native_df <- read_csv(here('data/sfo_trees/ca_native_spp.csv'))
```

How can we compare the California native species to those in our SF trees data? Add a column notes whether each tree is a CA native or not, and save as `sfo_trees_native` (include species info, legal status, plant date, and location). Then, count how many native vs. non-native trees for each legal status category, and save as `sfo_native_status`. Extension: include how many individual species in each category as well!

### Pseudocode

### Functional code

Hint: query the `help` page for `?separate` before you try to write functional code!

```{r ex 4b}
sfo_trees_native <- sfo_trees_df %>% 
  separate(species, into = c("spp_sci", "spp_common"), sep = " :: ") %>%
  select(starts_with('spp'), 'plant_date', 'legal_status', 'longitude', 'latitude') %>%
  mutate(ca_native = (spp_sci %in% ca_native_df$scientific_name))

sfo_native_status <- sfo_trees_native %>%
  group_by(legal_status, ca_native) %>%
  summarize(n_trees = n(),
            n_species = n_distinct(spp_sci))
```

## End part 1: Render this document!

When rendering a Quarto or R Markdown, R will create a new, clean environment - none of the variables or packages in current memory will be in this new environment.  This ensures that the script is completely self-contained - to help ensure reproducibility.

Then stage, commit, pull, push to GitHub!

------------------------------------------------------------------------

# Part 2: Analysis and quickie maps

Considering only Coast Live Oak and Monterey Pine, have tree planting preferences changed over time?

## Wrangling

Create a new dataframe that contains only Coast Live Oak and Monterey Pine observations (NOT all oaks and pines!), and include information on year and location. Call this `oak_pine_year_df`.

Then, determine whether there is a difference in when the trees have been planted.

### Pseudocode

### Functional Code

Note, here we need to use `lubridate::year()` to extract the year info from the `Date` info. We will do a lot more with `Date`-formatted data when we look at time series!

```{r part 2 analysis}

oak_pine_year_df <- sfo_trees_native %>% 
  filter(spp_sci %in% c('Quercus agrifolia', 'Pinus radiata')) %>%
  mutate(plant_year = year(plant_date))

t.test(plant_year ~ spp_sci, data = oak_pine_year_df)

ggplot(oak_pine_year_df) +
  geom_histogram(aes(x = plant_year), bins = 10) +
  facet_wrap(~ spp_sci, ncol = 1) +
  theme_minimal()

ggplot(oak_pine_year_df) +
  geom_point(aes(x = longitude, y = latitude, color = plant_year, shape = spp_sci))
```

## Creating a spatial map

You need `sf` ("Simple Features" geometry package) and `tmap` successfully attached to do this part. We'll convert lat/lon to spatial data (see that now there's a column called `geometry`), then we can use `geom_sf()` to plot. Here we are just touching on working with spatial data in R, and will explore this more in the coming weeks.

### Step 1: Convert the lat/lon to spatial points

Use `st_as_sf()` to convert to spatial coordinates (`_sf` suffix to remember that this is a simple features object; `sfo_` prefix here still means San Francisco!):

```{r map of sf oaks and pines}
oak_pine_sf <- oak_pine_year_df %>% 
  drop_na(longitude, latitude) %>% 
  st_as_sf(coords = c("longitude", "latitude")) # Convert to spatial coordinates

# But we need to set the coordinate reference system (CRS) so it's compatible with the street map of San Francisco we'll use as a "base layer":
st_crs(oak_pine_sf) <- 4326

# Then we can use `geom_sf`!

ggplot(data = oak_pine_sf) +
  geom_sf(aes(color = spp_sci)) +
  theme_minimal()
  
```

But that's not especially useful unless we have an actual map of SF to plot this on, right?

### Step 2: read in San Francisco road map

Read in the SF shapefile (data/sfo_map/tl_2017_06075_roads.shp):

```{r}
sfo_map <- read_sf(here("data", "sfo_map", "tl_2017_06075_roads.shp"))

st_transform(sfo_map, 4326)

ggplot(data = sfo_map) +
  geom_sf()
```

Now combine them:

```{r}
ggplot() +
  geom_sf(data = sfo_map,
          size = 0.1,
          color = "darkgray") +
  geom_sf(data = oak_pine_sf, 
          aes(color = spp_sci),
          size = 0.5) +
  theme_void() +
  labs(title = "Oaks and pines in San Francisco")
```

### Step 3: Now make it interactive!

```{r}
tmap_mode("view")

tm_shape(oak_pine_sf) + 
  tm_dots(col = 'spp_sci')
```

## Wrap up part 2

Make sure you render, stage, commit, pull, then push back to GitHub. Done!

------------------------------------------------------------------------

# Post-Lab Practice

Create a new repository in your GitHub account. Clone this into R Studio (no need to fork, it's already in your account).

In this new repository, create a new Quarto document and set it up:

-   Code folding (either `true` (fold, but hide it) or `show` (fold, but show it))
-   Echo code to the document, but suppress warnings and messages

Delete the template text, and create a new code chunk. Attach the `tidyverse` package. Attach the `gapminder` package (install it if you need to!).

## Analysis part 1:

* For each year, and for each continent, summarize the average per-capita GDP across all countries in the continent (hint: don't just average the country-level `gdpPercap` values!).
    * Plot these in a scatter plot.  
    * (Bonus, plot the country-level values as well, with a low alpha to fade them into the background)
    * Redo your scatter plot, but with a log transform on the per-capita GDP (either transform in the dataframe using `mutate`, or on the plot using `scale_y_log10()`)

## Analysis part 2:

Based on the plots, choose either the log-transformed or non-transformed data for the next part.

* On the original Gapminder data (not your summarized data), use linear regression to estimate the annual change in per capita GDP **OR** log(per capita GDP), accounting for continent.

## Followup questions:

* Write a sentence or two describing the trends you see in the plots.  Which model (log or not) did you choose and why?
* Write a sentence or two to explain the coefficients on your linear model output, including statistical significance.  
    * Which continent is the "reference" continent?
    * What does the "year" coefficient represent? hint: this is trickier if you went with the log-transformed model!
    * What do the coefficients on each continent represent?  Do these make intuitive sense?
* Does a linear model seem like a good way to approach these data?  Why or why not?

Render your Quarto document, and then stage, commit, and push back to your Github repository.  Make sure the updates are reflected online!

